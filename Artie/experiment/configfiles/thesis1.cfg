[experiment]
    # Random seed for the experiment (must be 32-bit unsigned integer)
    random-seed = 235238

# The Synthesizer section contains all the configuration options for the
# Articulatory Synthesis portions of the experiment.
[synthesizer]
    # The number of ms of all phonemes we will try to learn.
    phoneme-durations-ms = 500

    # When during the articulation the controller may change muscle activations.
    articulation-time-points-ms = 0 200 400

    # These are the [min, max, min, max, min, max] values for each articulator
    # for each time point.
    ####### THESE VALUES CREATE A KNOWN SOUND (when articulation-time-points-ms is 0, 100, 250, 500) #######
    #allowed-articulator-values = {
    #    Lungs:                      [0.2  0.2   0.0  0.0   0.0  0.0   0.0  0.0],
    #    Interarytenoid:             [0.5  0.5   0.5  0.5   0.5  0.5   0.5  0.5],
    #    Cricothyroid:               [0.0  0.0   0.0  0.0   0.0  0.0   0.0  0.0],
    #    Vocalis:                    [0.0  0.0   0.0  0.0   0.0  0.0   0.0  0.0],
    #    Thyroarytenoid:             [0.0  0.0   0.0  0.0   0.0  0.0   0.0  0.0],
    #    PosteriorCricoarytenoid:    [0.0  0.0   0.0  0.0   0.0  0.0   0.0  0.0],
    #    LateralCricoarytenoid:      [0.0  0.0   0.0  0.0   0.0  0.0   0.0  0.0],
    #    Stylohyoid:                 [0.0  0.0   0.0  0.0   0.0  0.0   0.0  0.0],
    #    Thyropharyngeus:            [0.0  0.0   0.0  0.0   0.0  0.0   0.0  0.0],
    #    LowerConstrictor:           [0.0  0.0   0.0  0.0   0.0  0.0   0.0  0.0],
    #    MiddleConstrictor:          [0.0  0.0   0.0  0.0   0.0  0.0   0.0  0.0],
    #    UpperConstrictor:           [0.0  0.0   0.0  0.0   0.0  0.0   0.0  0.0],
    #    Sphincter:                  [0.0  0.0   0.0  0.0   0.0  0.0   0.0  0.0],
    #    Hyoglossus:                 [0.0  0.0   0.0  0.0   0.0  0.0   0.0  0.0],
    #    Styloglossus:               [0.0  0.0   0.0  0.0   0.0  0.0   0.0  0.0],
    #    Genioglossus:               [0.0  0.0   0.0  0.0   0.0  0.0   0.0  0.0],
    #    UpperTongue:                [0.0  0.0   0.0  0.0   0.0  0.0   0.0  0.0],
    #    LowerTongue:                [0.0  0.0   0.0  0.0   0.0  0.0   0.0  0.0],
    #    TransverseTongue:           [0.0  0.0   0.0  0.0   0.0  0.0   0.0  0.0],
    #    VerticalTongue:             [0.0  0.0   0.0  0.0   0.0  0.0   0.0  0.0],
    #    Risorius:                   [0.0  0.0   0.0  0.0   0.0  0.0   0.0  0.0],
    #    OrbicularisOris:            [0.0  0.0   0.0  0.0   0.2  0.2   0.0  0.0],
    #    LevatorPalatini:            [1.0  1.0   1.0  1.0   1.0  1.0   1.0  1.0],
    #    TensorPalatini:             [0.0  0.0   0.0  0.0   0.0  0.0   0.0  0.0],
    #    Masseter:                   [0.0  0.0   0.0  0.0   0.7  0.7   0.0  0.0],
    #    Mylohyoid:                  [0.0  0.0   0.0  0.0   0.0  0.0   0.0  0.0],
    #    LateralPterygoid:           [0.0  0.0   0.0  0.0   0.0  0.0   0.0  0.0],
    #    Buccinator:                 [0.0  0.0   0.0  0.0   0.0  0.0   0.0  0.0],
    #    }

    # These are the [min, max, min, max] values for each articulator
    # for each time point. See synthesizer.py for which of these are in which
    # articulator group.
    # This matrix specifies the total limits. If annealing is turned on,
    # we zero out portions of the matrix during specific portions of phase 1
    # training. Regardless of whether annealing is on or off for phase 0,
    # we zero out all but the laryngeal group during phase 0.
    allowed-articulator-values = {
        Lungs:                      [0.2  0.2   0.0  0.0   0.0  0.0],
        Interarytenoid:             [0.5  0.5   0.5  0.5   0.5  0.5],
        # This muscle changes the pitch (frequency)
        Cricothyroid:               [-1.0  1.0   -1.0  1.0   -1.0  1.0],
        # This and thyroarytenoid work together to do something I guess
        Vocalis:                    [-1.0  1.0   -1.0  1.0   -1.0  1.0],
        Thyroarytenoid:             [-1.0  1.0   -1.0  1.0   -1.0  1.0],
        # These two have to do with breathing maybe?
        PosteriorCricoarytenoid:    [0.0  0.0   0.0  0.0   0.0  0.0],
        LateralCricoarytenoid:      [0.0  0.0   0.0  0.0   0.0  0.0],
        # Group of muscles used in swallowing. Less so in speech. Set to zeros.
        Stylohyoid:                 [0.0  0.0   0.0  0.0   0.0  0.0],
        Thyropharyngeus:            [0.0  0.0   0.0  0.0   0.0  0.0],
        LowerConstrictor:           [0.0  0.0   0.0  0.0   0.0  0.0],
        MiddleConstrictor:          [0.0  0.0   0.0  0.0   0.0  0.0],
        UpperConstrictor:           [0.0  0.0   0.0  0.0   0.0  0.0],
        Sphincter:                  [0.0  0.0   0.0  0.0   0.0  0.0],
        # Helps with tongue (all glossus muscles)
        Hyoglossus:                 [0.0  0.0   0.0  0.0   0.0  0.0],
        Styloglossus:               [0.0  0.5   0.0  0.5   0.0  0.5],
        Genioglossus:               [0.0  0.0   0.0  0.0   0.0  0.0],
        # Tongue!
        UpperTongue:                [-1.0  0.0   -1.0  0.0   -1.0  0.0],
        LowerTongue:                [-1.0  0.0   -1.0  0.0   -1.0  0.0],
        TransverseTongue:           [0.0  0.0   0.0  0.0   0.0  0.0],
        VerticalTongue:             [0.0  0.0   0.0  0.0   0.0  0.0],
        # Helpful for smiling
        Risorius:                   [0.0  0.0   0.0  0.0   0.0  0.0],
        # Lip-puckering
        OrbicularisOris:            [0.5  1.0   0.5  1.0   0.5  1.0],
        # Soft-palate (responsible for nasalisation) - same with tensor palatini
        LevatorPalatini:            [-1.0  1.0   -1.0  1.0   -1.0  1.0],
        TensorPalatini:             [-1.0  1.0   -1.0  1.0   -1.0  1.0],
        # Helps to open/close the jaw
        Masseter:                   [-0.5  0.0   -0.5  0.0   -0.5  0.0],
        # Important for moving the bottom of the mouth and the tongue
        Mylohyoid:                  [-1.0  1.0   -1.0  1.0   -1.0  1.0],
        # Helps to open/close the jaw
        LateralPterygoid:           [0.0  0.0   0.0  0.0   0.0  0.0],
        # Helpful in puckering and whistling
        Buccinator:                 [0.0  0.0   0.0  0.0   0.0  0.0],
        }

    # The number of agents to create as part of the genetic algorithm for phase 0 (loudness training)
    nagents-phase0 = 100

    # The number of agents to create as part of the genetic algorithm for phase 1 (mimicking)
    nagents-phase1 = 400

    # The number of workers to use in parallel for training the genetic algorithm in phase 0
    nworkers-phase0 = 12

    # The number of workers to use in parallel for training the genetic algorithm in phase 1
    nworkers-phase1 = 12

    # The number of generations of agents to simulate for the genetic algorithm in phase 0. Can be None.
    # Either this or fitness-target-phase0 must be specified, however.
    niterations-phase0 = 12

    # The number of generations of agents to simulate for the genetic algorithm in phase 1. Can be None.
    # Either this or fitness-target-phase1 must be specified, however.
    # If anneal-during-phase1 is True, this can be a list, in which case, it should contain the number
    # of iterations for each group of articulators. If annealing is turned on and this is not a list,
    # we will do this many iterations per anneal.
    # Jaw, Nasal, Lingual support, Lingual main, Labial
    niterations-phase1 = 10 10 20 10 10

    # The target fitness value to converge on in the genetic algorithm in phase 0. Can be None.
    # Either this or niterations-phase0 must be specified, however.
    fitness-target-phase0 = None

    # The target fitness value to converge on in the genetic algorithm in phase 1. Can be None.
    # Either this or niterations-phase1 must be specified, however.
    # If anneal-during-phase1 is True, this can be a list, in which case, it should contain the fitness targets
    # for each group of articulators. If annealing is turned on and this is not a list,
    # we will use this for each articulator group.
    fitness-target-phase1 = None

    # The top x percent of each generation is chosen for breeding in the genetic algorithm
    fraction-of-generation-to-select-phase0 = 0.5

    # The top x percent of each generation is chosen for breeding in the genetic algorithm
    fraction-of-generation-to-select-phase1 = 0.5

    # X percent of each generation is subject to mutation.
    fraction-of-generation-to-mutate-phase0 = 0.10

    # X percent of each generation is subject to mutation.
    fraction-of-generation-to-mutate-phase1 = 0.10

    # Anneal the limits based on best agents from phase 0?
    anneal-after-phase0 = True

    # Anneal during phase 1? Note that annealing during phase 1 is different from how it is
    # implemented in phase 0. During phase 1 (if annealing is on), we zero the limits
    # on all muscles except the laryngeal group (which is either randomized or pretrained from phase 0),
    # and the jaw group. We then let the algorithm explore for some number of steps, before we
    # anneal the jaw group and move on to opening up the lingual group. Then we anneal the lingual
    # group and we open up the labial group.
    anneal-during-phase1 = True

# The autoencoder section contains all the configuration options for the
# variational autoencoder portions of the experiment.
[autoencoder]
    # The input shape that the encoder layer is expecting. This will depend on the ms per spectrogram,
    # the overlap, and the sampling frequency of the audio.
    input_shape = 241 20 1

    # The dimensionality of the embedding space
    nembedding_dims = 2

    # The Keras Optimizer to use for the autoencoder
    optimizer = adadelta

    # The loss function to use for the autoencoder. See the VAE for available values.
    loss = mse

    # The root of the preprocessed data directory. We will grab data from here and feed it into the autoencoder to train it.
    preprocessed_data_root = /media/max/seagate8TB/thesis_audio/test_spectrogram_images/hundred_thousand

    # The number of spectrograms to batch at a time
    batchsize = 32

    # The number of workers to use to train
    nworkers = 4

    # The number of epochs to train. Note that we define how long an epoch is manually.
    nepochs = 2

    # The number of steps per epoch. (Should be num_samples / batchsize)
    steps_per_epoch = 300

    # The path to save/load the autoencoder weights. A timestamp and .h5 extension will be appended to this base name.
    weights_path = /home/max/repos/ArtieInfant/Artie/models/vae/test

    # Save a before/after image after every epoch if True.
    visualize = False

    # Whether or not we should visualize the autoencoder with TensorBoard. If so, this must be a valid directory.
    tbdir = /home/max/repos/ArtieInfant/Artie/tensorboarddir

# The Preprocessing section contains all the configuration options for
# the preprocessing pipeline. This pipeline attempts to take completely raw
# WAV files and get them into shape for the RL and VAE algorithms.
[preprocessing]
    # Root of all the audio that you want to preprocess
    root = /media/max/seagate8TB/thesis_audio/gold_data_do_not_modify

    # Folder where all the preprocessed audio will be dumped
    destination = /media/max/seagate8TB/thesis_audio/preprocessed_gold_data

    # Number of worker processes to use to preprocess
    nworkers = 4

    # Resample the audio to this sample rate.
    sample_rate_hz = 32000.0

    # For each file that we find, we preprocess it only with this probability. Useful for debugging/testing.
    fraction_to_preprocess = 0.01

    # Number of channels to resample into. Only mono makes any sense for this experiment.
    nchannels = 1

    # Bytewidth of each sample to resample to.
    bytewidth = 2

    # Seconds each piece of audio should be diced down to (before preprocessing each one).
    dice_to_seconds = 600

    # -- Baby Detector --
    # Each of the following parameters must match exactly the values you used to train the model.

    # The sampling rate that the baby detector expects the data to be in.
    baby_detector_sample_rate_hz = 22050.0

    # The byte-width the baby detector expects the data to be in.
    baby_detector_sample_width_bytes = 2

    # The number of ms of audio data to feed into the baby detector at a time.
    baby_detector_ms = 300.0

    # The type of model to create for the baby detector. Allowable options are ['fft', 'spec'] for an FFT model or a Spectrogram model.
    baby_detector_model_type = fft

    # Only used if 'baby_detector_model_type' is "spec". The ms per window.
    baby_detector_window_length_ms = 30.0

    # Only used if 'baby_detector_model_type' is "spec". The fraction of each window to overlap.
    baby_detector_overlap = 0.125

    # Only used if 'baby_detector_model_type' is "spec". The shape of the spectrogram data to expect.
    baby_detector_spectrogram_shape = [1, 2]

    # Probability of observing a transition from baby sound to no baby sound in 'baby_detector_ms' segment of data.
    baby_detector_p_yes_to_no = 0.2

    # Probability of observing a transition from no baby sound to baby sound in 'baby_detector_ms' segment of data.
    baby_detector_p_no_to_yes = 0.1

    # The P(reality=1 | model output=1)
    baby_detector_positive_predictive_value = 0.5

    # The P(reality=1 | model output=0)
    baby_detector_negative_predictive_value = 0.5

    # The typical length of a baby event in seconds
    baby_detector_event_length_s = 1.5

    # The raw probability of baby noise being present in any given 'baby_detector_ms' segment of data.
    baby_detector_raw_yes = 0.3

    # -- Language Detector --
    # Each of the following parameters must match exactly the values you used to train the model.

    # The sampling rate that the language detector expects the data to be in.
    language_detector_sample_rate_hz = 22050.0

    # The byte-width the language detector expects the data to be in.
    language_detector_sample_width_bytes = 2

    # The number of ms of audio data to feed into the language detector at a time.
    language_detector_ms = 300.0

    # The type of model to create for the language detector. Allowable options are ['fft', 'spec'] for an FFT model or a Spectrogram model.
    language_detector_model_type = fft

    # Only used if 'language_detector_model_type' is "spec". The ms per window.
    language_detector_window_length_ms = 30.0

    # Only used if 'language_detector_model_type' is "spec". The fraction of each window to overlap.
    language_detector_overlap = 0.125

    # Only used if 'language_detector_model_type' is "spec". The shape of the spectrogram data to expect.
    language_detector_spectrogram_shape = [1, 2]

    # Probability of observing a transition from Chinese to not Chinese in 'language_detector_ms' segment of data.
    language_detector_p_yes_to_no = 0.2

    # Probability of observing a transition from no Chinese to Chinese in 'language_detector_ms' segment of data.
    language_detector_p_no_to_yes = 0.1

    # The P(reality=1 | model output=1)
    language_detector_positive_predictive_value = 0.5

    # The P(reality=1 | model output=0)
    language_detector_negative_predictive_value = 0.5

    # The typical length of a Chinese event in seconds
    language_detector_event_length_s = 1.5

    # The raw probability of Chinese being present in any given 'language_detector_ms' segment of data.
    language_detector_raw_yes = 0.3

    # -- Spectrograms --

    # Where to put the images of the spectrograms
    images_destination = /media/max/seagate8TB/thesis_audio/filterbank_images/all/pointless_subdir

    # Were to put the wav files that we used to create the spectrograms
    folder_to_save_wavs = /media/max/seagate8TB/thesis_audio/filterbank_audio/all/

    # Use a filterbank?
    use_filterbank = False

    # Sample rate (in Hz) to resample to before creating the spectrograms
    spectrogram_sample_rate_hz = 16000.0

    # Seconds per spectrogram (this should almost certainly be the same value as phoneme-durations-ms)
    seconds_per_spectrogram = 0.5

    # Length (in seconds) of each spectrogram window
    spectrogram_window_length_s = 0.03

    # Fraction of overlap between each spectrogram window
    spectrogram_window_overlap = 0.2
